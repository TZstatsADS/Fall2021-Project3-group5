{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "educational-garbage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import numpy as np\n",
    "import time \n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Flatten, MaxPooling2D, Activation, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop, Nadam\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "thermal-street",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook uses TensorFlow Version 2.2.0\n",
      "And Python Version:\n",
      "Python 3.7.9\r\n"
     ]
    }
   ],
   "source": [
    "print(f\"This notebook uses TensorFlow Version {tf.__version__}\")\n",
    "print(\"And Python Version:\")\n",
    "!python --version "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-andrew",
   "metadata": {},
   "source": [
    "## 1. Load the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-forum",
   "metadata": {},
   "source": [
    "For the project, we provide a training set with 50000 images in the directory `../data/images/` with:\n",
    "- noisy labels for all images provided in `../data/noisy_label.csv`;\n",
    "- clean labels for the first 10000 images provided in `../data/clean_labels.csv`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "supreme-theorem",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [DO NOT MODIFY THIS CELL]\n",
    "\n",
    "# load the images\n",
    "n_img = 50000\n",
    "n_noisy = 40000\n",
    "n_clean_noisy = n_img - n_noisy\n",
    "imgs = np.empty((n_img,32,32,3))\n",
    "for i in range(n_img):\n",
    "    img_fn = f'../data/images/{i+1:05d}.png'\n",
    "    imgs[i,:,:,:]=cv2.cvtColor(cv2.imread(img_fn),cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# load the labels\n",
    "clean_labels = np.genfromtxt('../data/clean_labels.csv', delimiter=',', dtype=\"int8\")\n",
    "noisy_labels = np.genfromtxt('../data/noisy_labels.csv', delimiter=',', dtype=\"int8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-breakfast",
   "metadata": {},
   "source": [
    "For illustration, we present a small subset (of size 8) of the images with their clean and noisy labels in `clean_noisy_trainset`. You are encouraged to explore more characteristics of the label noises on the whole dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "valued-finish",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean labels:\n",
      " frog truck truck  deer   car   car  bird horse\n",
      "Noisy labels:\n",
      "  cat   dog truck  frog   dog  ship  bird  deer\n"
     ]
    }
   ],
   "source": [
    "# [DO NOT MODIFY THIS CELL]\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax1 = fig.add_subplot(2,4,1)\n",
    "ax1.imshow(imgs[0]/255)\n",
    "ax2 = fig.add_subplot(2,4,2)\n",
    "ax2.imshow(imgs[1]/255)\n",
    "ax3 = fig.add_subplot(2,4,3)\n",
    "ax3.imshow(imgs[2]/255)\n",
    "ax4 = fig.add_subplot(2,4,4)\n",
    "ax4.imshow(imgs[3]/255)\n",
    "ax1 = fig.add_subplot(2,4,5)\n",
    "ax1.imshow(imgs[4]/255)\n",
    "ax2 = fig.add_subplot(2,4,6)\n",
    "ax2.imshow(imgs[5]/255)\n",
    "ax3 = fig.add_subplot(2,4,7)\n",
    "ax3.imshow(imgs[6]/255)\n",
    "ax4 = fig.add_subplot(2,4,8)\n",
    "ax4.imshow(imgs[7]/255)\n",
    "\n",
    "# The class-label correspondence\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# print clean labels\n",
    "print('Clean labels:')\n",
    "print(' '.join('%5s' % classes[clean_labels[j]] for j in range(8)))\n",
    "# print noisy labels\n",
    "print('Noisy labels:')\n",
    "print(' '.join('%5s' % classes[noisy_labels[j]] for j in range(8)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-indianapolis",
   "metadata": {},
   "source": [
    "## 2. The predictive model\n",
    "\n",
    "We consider a baseline model directly on the noisy dataset without any label corrections. RGB histogram features are extracted to fit a logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-template",
   "metadata": {},
   "source": [
    "### 2.1. Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "behavioral-enough",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [DO NOT MODIFY THIS CELL]\n",
    "# RGB histogram dataset construction\n",
    "no_bins = 6\n",
    "bins = np.linspace(0,255,no_bins) # the range of the rgb histogram\n",
    "target_vec = np.empty(n_img)\n",
    "feature_mtx = np.empty((n_img,3*(len(bins)-1)))\n",
    "i = 0\n",
    "for i in range(n_img):\n",
    "    # The target vector consists of noisy labels\n",
    "    target_vec[i] = noisy_labels[i]\n",
    "    \n",
    "    # Use the numbers of pixels in each bin for all three channels as the features\n",
    "    feature1 = np.histogram(imgs[i][:,:,0],bins=bins)[0] \n",
    "    feature2 = np.histogram(imgs[i][:,:,1],bins=bins)[0]\n",
    "    feature3 = np.histogram(imgs[i][:,:,2],bins=bins)[0]\n",
    "    \n",
    "    # Concatenate three features\n",
    "    feature_mtx[i,] = np.concatenate((feature1, feature2, feature3), axis=None)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "powerful-yesterday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [DO NOT MODIFY THIS CELL]\n",
    "# Train a logistic regression model \n",
    "clf = LogisticRegression(random_state=0).fit(feature_mtx, target_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "global-status",
   "metadata": {},
   "source": [
    "For the convenience of evaluation, we write the following function `predictive_model` that does the label prediction. **For your predictive model, feel free to modify the function, but make sure the function takes an RGB image of numpy.array format with dimension $32\\times32\\times3$  as input, and returns one single label as output.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "according-theology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [DO NOT MODIFY THIS CELL]\n",
    "def baseline_model(image):\n",
    "    '''\n",
    "    This is the baseline predictive model that takes in the image and returns a label prediction\n",
    "    '''\n",
    "    feature1 = np.histogram(image[:,:,0],bins=bins)[0]\n",
    "    feature2 = np.histogram(image[:,:,1],bins=bins)[0]\n",
    "    feature3 = np.histogram(image[:,:,2],bins=bins)[0]\n",
    "    feature = np.concatenate((feature1, feature2, feature3), axis=None).reshape(1,-1)\n",
    "    return clf.predict(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-multimedia",
   "metadata": {},
   "source": [
    "### 2.2. Model I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-verse",
   "metadata": {},
   "source": [
    "For model I, we use a basic CNN structure: two 2D convolutional layers, a max pooling layer, a flatten layer, a dense layer and the classficication layer. \n",
    "\n",
    "For the optimizer, we use Nadam and the learning rate is 0.001.\n",
    "\n",
    "We use data augmentation in order to reduce overfitting. The data augmentation also increases the amount of data as it adds modified copies of the orginial data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-polish",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "distinguished-graham",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the images and split the data into training and validation set (0.9 and 0.1)\n",
    "shuff_imgs, target_vec = shuffle(imgs, noisy_labels, random_state=0)\n",
    "img_train, img_test, y_train, y_test = train_test_split(shuff_imgs, target_vec, test_size=0.10, random_state=42)\n",
    "# one-hot encoded vectors\n",
    "y_train = np.eye(10)[y_train]\n",
    "y_test = np.eye(10)[y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "intended-designer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelI():\n",
    "    \n",
    "    # Simple CNN with 2 convolutional layers, max pooling, flatten layer and dense layer\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    # COMPILE\n",
    "    opt= Nadam(\n",
    "    learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Nadam\")\n",
    "    \n",
    "    # compile\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_model(model, img_train, y_train, img_test, y_test, output_fn, epochs = 10 ):\n",
    "    \n",
    "    # generate image data with data augmentation \n",
    "    train_gen = ImageDataGenerator(\n",
    "        featurewise_center=True, # set the mean of the inputs to 0 over the dataset\n",
    "        featurewise_std_normalization=True, # divide the inputs by standard deviation of the dataset\n",
    "        rotation_range=20, # degree range for random rotations\n",
    "        width_shift_range=0.2, # the fraction of total width\n",
    "        height_shift_range=0.2, # the fraction of total height\n",
    "        horizontal_flip=True) # flip the inputs horizontally randomly\n",
    "    \n",
    "    train_gen.fit(img_train)\n",
    "    \n",
    "    test_gen = ImageDataGenerator(\n",
    "        featurewise_center=True,\n",
    "        featurewise_std_normalization=True\n",
    "    )\n",
    "    test_gen.fit(img_train)\n",
    "    \n",
    "    # save the weights\n",
    "    file_path = f\"../output/{output_fn}\"\n",
    "    checkpoint = ModelCheckpoint(file_path, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "    \n",
    "    # fits the model on batches with data augmentation:\n",
    "    model.fit(train_gen.flow(img_train, y_train, batch_size=128),\n",
    "             validation_data=train_gen.flow(img_test, y_test, batch_size=12),\n",
    "             callbacks=callbacks_list,\n",
    "              epochs=epochs)\n",
    "    \n",
    "    return model, test_gen \n",
    "\n",
    "def model_I(image):\n",
    "    '''\n",
    "    This function should takes in the image of dimension 32*32*3 as input and returns a label prediction\n",
    "    '''\n",
    "    # load the model weights\n",
    "    model1 = modelI() \n",
    "    model1.load_weights(\"../output/modelI.h5\")\n",
    "    \n",
    "    # predict\n",
    "    pred = model1.predict(data_genI.flow(image))\n",
    "    pred_class = np.argmax(pred)\n",
    "    \n",
    "    return pred_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "social-landscape",
   "metadata": {},
   "source": [
    "Train the CNN for 6 epochs - would train for more, but only saw 2-3% gains, so reduced to 6 for the runtime reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "distinct-jones",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "352/352 [==============================] - ETA: 0s - loss: 2.2729 - accuracy: 0.1613\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.17940, saving model to ../output/modelI.h5\n",
      "352/352 [==============================] - 85s 243ms/step - loss: 2.2729 - accuracy: 0.1613 - val_loss: 2.2531 - val_accuracy: 0.1794\n",
      "Epoch 2/6\n",
      "352/352 [==============================] - ETA: 0s - loss: 2.2418 - accuracy: 0.1900\n",
      "Epoch 00002: val_accuracy improved from 0.17940 to 0.18980, saving model to ../output/modelI.h5\n",
      "352/352 [==============================] - 82s 233ms/step - loss: 2.2418 - accuracy: 0.1900 - val_loss: 2.2400 - val_accuracy: 0.1898\n",
      "Epoch 3/6\n",
      "352/352 [==============================] - ETA: 0s - loss: 2.2307 - accuracy: 0.2042\n",
      "Epoch 00003: val_accuracy improved from 0.18980 to 0.20220, saving model to ../output/modelI.h5\n",
      "352/352 [==============================] - 81s 231ms/step - loss: 2.2307 - accuracy: 0.2042 - val_loss: 2.2310 - val_accuracy: 0.2022\n",
      "Epoch 4/6\n",
      "352/352 [==============================] - ETA: 0s - loss: 2.2226 - accuracy: 0.2101\n",
      "Epoch 00004: val_accuracy improved from 0.20220 to 0.20480, saving model to ../output/modelI.h5\n",
      "352/352 [==============================] - 83s 236ms/step - loss: 2.2226 - accuracy: 0.2101 - val_loss: 2.2298 - val_accuracy: 0.2048\n",
      "Epoch 5/6\n",
      "352/352 [==============================] - ETA: 0s - loss: 2.2146 - accuracy: 0.2161\n",
      "Epoch 00005: val_accuracy improved from 0.20480 to 0.22020, saving model to ../output/modelI.h5\n",
      "352/352 [==============================] - 92s 261ms/step - loss: 2.2146 - accuracy: 0.2161 - val_loss: 2.2145 - val_accuracy: 0.2202\n",
      "Epoch 6/6\n",
      "352/352 [==============================] - ETA: 0s - loss: 2.2085 - accuracy: 0.2221\n",
      "Epoch 00006: val_accuracy did not improve from 0.22020\n",
      "352/352 [==============================] - 89s 252ms/step - loss: 2.2085 - accuracy: 0.2221 - val_loss: 2.2180 - val_accuracy: 0.2192\n"
     ]
    }
   ],
   "source": [
    "# record the computational time\n",
    "start = time.time()\n",
    "# train model 1\n",
    "model = modelI()\n",
    "model, data_genI = train_model(model, img_train, y_train, img_test, y_test, \"modelI.h5\", epochs = 6)\n",
    "end = time.time() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "presidential-soundtrack",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Model I training time: 612.0104508399963\n"
     ]
    }
   ],
   "source": [
    "print( f\"Total Model I training time: {end-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "promising-criterion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 5s 16ms/step - loss: 1.7399 - accuracy: 0.4915\n",
      "Model I Clean Labels Loss, Accuracy: [1.7399446964263916, 0.49149999022483826]\n"
     ]
    }
   ],
   "source": [
    "# clean images and labels\n",
    "img_cl = imgs[:10000]\n",
    "y_cl = np.eye(10)[clean_labels]\n",
    "\n",
    "# estimate the model accuracy on using clean labels\n",
    "cl_metrics = model.evaluate(data_genI.flow(img_cl, y_cl))\n",
    "\n",
    "print(f\"Model I Clean Labels Loss, Accuracy: {cl_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-desktop",
   "metadata": {},
   "source": [
    "### 2.3. Model II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-lecture",
   "metadata": {},
   "source": [
    "For Model II, we train a label cleaning network that follows a similar architecture as the paper. We used a pre-trained CNN (VGG16) for the base network, and tried to match the rest of the architecture to the paper. We make the last layer of VGG16 to be trainable in order to avoid overfitting.\n",
    "\n",
    "The label network is only trained for 6 epochs, as it is time intensive, but performance could be increased by training for more epochs. \n",
    "\n",
    "We then use the label cleaining network to predict new labels for the 40000 noisy images, and use the new labels along with the 10000 clean labels to retrain a new CNN that has the same architecture as Model 1. Overall accuracy increased. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-gateway",
   "metadata": {},
   "source": [
    "### Label Cleaning Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "professional-wheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get both clean and noisy labels for the first 10,000 images \n",
    "clean = np.eye(10)[clean_labels]\n",
    "noisy = np.eye(10)[noisy_labels[:10000]]\n",
    "clean_imgs = imgs[:10000]/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aware-induction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Lambda\n",
    "\n",
    "#Custom loss function for comparing predicted class to clean label used for training label network \n",
    "def label_loss(y_true, y_pred):\n",
    "    # L1 distance between true labels and predicted labels\n",
    "    loss = K.abs(y_true - y_pred)\n",
    "    loss = K.sum(loss, axis = 1)\n",
    "    loss = K.sum(loss)\n",
    "    return loss \n",
    "    \n",
    "def label_nn():\n",
    "    # input layer\n",
    "    img_input = Input(shape=(32, 32, 3))\n",
    "    noisy_label = Input(shape = (10))\n",
    "    \n",
    "    # transfer learning - using VGG16 here\n",
    "    base = VGG16(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        input_shape=(32,32,3),\n",
    "        pooling='max'\n",
    "    )\n",
    "\n",
    "    # make the last layer of VGG16 trainable\n",
    "    base.trainable = False\n",
    "    base.get_layer('block5_conv3').trainable = True\n",
    "\n",
    "    # use VGG16 as the base model\n",
    "    img_vec = base(img_input)\n",
    "\n",
    "    noisy_l = Dense(10)(noisy_label)\n",
    "    img_vec = Dense(256)(img_vec)\n",
    "    \n",
    "    # concatenate noisy labels and image features\n",
    "    x = Concatenate(axis=-1)([noisy_l, img_vec])\n",
    "    x = Dense(256, activation = 'relu')(x)\n",
    "    out = Dense(10, activation = 'softmax')(x)\n",
    "\n",
    "    model = Model([img_input, noisy_label], out)\n",
    "    # compile\n",
    "    model.compile(loss=label_loss, metrics=['acc'], optimizer=RMSprop(0.001))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dirty-hawaii",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 2s 0us/step\n",
      "Epoch 1/6\n",
      "79/79 [==============================] - 59s 746ms/step - loss: 190.2200 - acc: 0.2704\n",
      "Epoch 2/6\n",
      "79/79 [==============================] - 58s 730ms/step - loss: 152.7823 - acc: 0.4278\n",
      "Epoch 3/6\n",
      "79/79 [==============================] - 64s 811ms/step - loss: 125.1476 - acc: 0.5362\n",
      "Epoch 4/6\n",
      "79/79 [==============================] - 53s 665ms/step - loss: 115.6083 - acc: 0.5692\n",
      "Epoch 5/6\n",
      "79/79 [==============================] - 58s 731ms/step - loss: 107.7472 - acc: 0.5920\n",
      "Epoch 6/6\n",
      "79/79 [==============================] - 54s 681ms/step - loss: 102.6685 - acc: 0.6146\n"
     ]
    }
   ],
   "source": [
    "# record the computational time\n",
    "start = time.time()\n",
    "model = label_nn()\n",
    "# train the label model\n",
    "model.fit([clean_imgs, noisy], clean, batch_size = 128, epochs = 6)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "annoying-consciousness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Label Network training Time: 353.5897169113159\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Label Network training Time: {end-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "environmental-symposium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save(\"../output/model_labelclean.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "small-credit",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict new labels for noisy set\n",
    "noisy_imgs = imgs[10000:]/255\n",
    "noisy_l = np.eye(10)[noisy_labels[10000:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "other-mexican",
   "metadata": {},
   "outputs": [],
   "source": [
    "# record the computational time\n",
    "start = time.time()\n",
    "# predict labels\n",
    "new_pred = model.predict([noisy_imgs, noisy_l])\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "answering-danger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Label Network prediction Time: 239.63977193832397\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Label Network prediction Time: {end-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "known-quantum",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up label vectors\n",
    "row_maxes = new_pred.argmax(axis=1)\n",
    "new_labels = np.eye(10)[row_maxes]\n",
    "\n",
    "#Create new train set from clean images and new pred labels\n",
    "upd_imgs = imgs\n",
    "upd_labels = np.concatenate((np.eye(10)[clean_labels], new_labels), axis=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organizational-actress",
   "metadata": {},
   "source": [
    "### Train Model II with clean labels and new labels from label network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "diagnostic-leader",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the images and split the data into training and validation set (0.9 and 0.1)\n",
    "shuff_imgs, target_vec = shuffle(upd_imgs, upd_labels, random_state=0)\n",
    "img_train, img_test, y_train, y_test = train_test_split(shuff_imgs,target_vec, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "suffering-federation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "352/352 [==============================] - ETA: 0s - loss: 1.7120 - accuracy: 0.3909\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.44380, saving model to ../output/modelII.h5\n",
      "352/352 [==============================] - 85s 243ms/step - loss: 1.7120 - accuracy: 0.3909 - val_loss: 1.5608 - val_accuracy: 0.4438\n",
      "Epoch 2/10\n",
      "352/352 [==============================] - ETA: 0s - loss: 1.4799 - accuracy: 0.4776\n",
      "Epoch 00002: val_accuracy improved from 0.44380 to 0.50140, saving model to ../output/modelII.h5\n",
      "352/352 [==============================] - 83s 236ms/step - loss: 1.4799 - accuracy: 0.4776 - val_loss: 1.4234 - val_accuracy: 0.5014\n",
      "Epoch 3/10\n",
      "352/352 [==============================] - ETA: 0s - loss: 1.3964 - accuracy: 0.5095\n",
      "Epoch 00003: val_accuracy improved from 0.50140 to 0.51280, saving model to ../output/modelII.h5\n",
      "352/352 [==============================] - 84s 239ms/step - loss: 1.3964 - accuracy: 0.5095 - val_loss: 1.4071 - val_accuracy: 0.5128\n",
      "Epoch 4/10\n",
      "352/352 [==============================] - ETA: 0s - loss: 1.3436 - accuracy: 0.5290\n",
      "Epoch 00004: val_accuracy improved from 0.51280 to 0.54080, saving model to ../output/modelII.h5\n",
      "352/352 [==============================] - 83s 237ms/step - loss: 1.3436 - accuracy: 0.5290 - val_loss: 1.3296 - val_accuracy: 0.5408\n",
      "Epoch 5/10\n",
      "352/352 [==============================] - ETA: 0s - loss: 1.3134 - accuracy: 0.5409\n",
      "Epoch 00005: val_accuracy did not improve from 0.54080\n",
      "352/352 [==============================] - 92s 262ms/step - loss: 1.3134 - accuracy: 0.5409 - val_loss: 1.3154 - val_accuracy: 0.5402\n",
      "Epoch 6/10\n",
      "352/352 [==============================] - ETA: 0s - loss: 1.2911 - accuracy: 0.5491\n",
      "Epoch 00006: val_accuracy improved from 0.54080 to 0.55060, saving model to ../output/modelII.h5\n",
      "352/352 [==============================] - 86s 246ms/step - loss: 1.2911 - accuracy: 0.5491 - val_loss: 1.2947 - val_accuracy: 0.5506\n",
      "Epoch 7/10\n",
      "352/352 [==============================] - ETA: 0s - loss: 1.2701 - accuracy: 0.5567\n",
      "Epoch 00007: val_accuracy did not improve from 0.55060\n",
      "352/352 [==============================] - 83s 235ms/step - loss: 1.2701 - accuracy: 0.5567 - val_loss: 1.2914 - val_accuracy: 0.5456\n",
      "Epoch 8/10\n",
      "352/352 [==============================] - ETA: 0s - loss: 1.2567 - accuracy: 0.5620\n",
      "Epoch 00008: val_accuracy improved from 0.55060 to 0.56140, saving model to ../output/modelII.h5\n",
      "352/352 [==============================] - 90s 256ms/step - loss: 1.2567 - accuracy: 0.5620 - val_loss: 1.2722 - val_accuracy: 0.5614\n",
      "Epoch 9/10\n",
      "352/352 [==============================] - ETA: 0s - loss: 1.2452 - accuracy: 0.5642\n",
      "Epoch 00009: val_accuracy did not improve from 0.56140\n",
      "352/352 [==============================] - 81s 232ms/step - loss: 1.2452 - accuracy: 0.5642 - val_loss: 1.2683 - val_accuracy: 0.5558\n",
      "Epoch 10/10\n",
      "352/352 [==============================] - ETA: 0s - loss: 1.2248 - accuracy: 0.5742\n",
      "Epoch 00010: val_accuracy improved from 0.56140 to 0.56980, saving model to ../output/modelII.h5\n",
      "352/352 [==============================] - 82s 233ms/step - loss: 1.2248 - accuracy: 0.5742 - val_loss: 1.2541 - val_accuracy: 0.5698\n"
     ]
    }
   ],
   "source": [
    "# record the computational time\n",
    "start = time.time()\n",
    "# train model 2\n",
    "modelII = modelI()\n",
    "modelII, data_genII = train_model(modelII, img_train, y_train, img_test, y_test,\"modelII.h5\", 10)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "colonial-angola",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Model II training Time: 612.0104508399963\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Model II training Time: {end-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "overall-denial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 4s 12ms/step - loss: 1.1268 - accuracy: 0.6162\n",
      "Model II Clean Labels Loss, Accuracy: [1.1267924308776855, 0.6161999702453613]\n"
     ]
    }
   ],
   "source": [
    "# estimate the model accuracy on using clean labels\n",
    "clean_metrics = modelII.evaluate(data_genII.flow(imgs[:10000], np.eye(10)[clean_labels]))\n",
    "print(f\"Model II Clean Labels Loss, Accuracy: {clean_metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "internal-contamination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ADD WEAKLY SUPERVISED LEARNING FEATURE TO MODEL I]\n",
    "\n",
    "def model_II(image):\n",
    "    '''\n",
    "    This function should takes in the image of dimension 32*32*3 as input and returns a label prediction\n",
    "    '''\n",
    "    # load the model weights\n",
    "    model2 = modelI() \n",
    "    model2.load_weights(\"../output/modelII.h5\")\n",
    "    \n",
    "    # predict\n",
    "    pred = model2.predict(data_genII.flow(image))\n",
    "    pred_class = np.argmax(pred, axis = 1)\n",
    "    \n",
    "    return pred_class "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-movie",
   "metadata": {},
   "source": [
    "## 3. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-notebook",
   "metadata": {},
   "source": [
    "For assessment, we will evaluate your final model on a hidden test dataset with clean labels by the `evaluation` function defined as follows. Although you will not have the access to the test set, the function would be useful for the model developments. For example, you can split the small training set, using one portion for weakly supervised learning and the other for validation purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laughing-association",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [DO NOT MODIFY THIS CELL]\n",
    "def evaluation(model, test_labels, test_imgs):\n",
    "    y_true = test_labels\n",
    "    y_pred = []\n",
    "    for image in test_imgs:\n",
    "        y_pred.append(model(image))\n",
    "    print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-certification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [DO NOT MODIFY THIS CELL]\n",
    "# This is the code for evaluating the prediction performance on a testset\n",
    "# You will get an error if running this cell, as you do not have the testset\n",
    "# Nonetheless, you can create your own validation set to run the evlauation\n",
    "n_test = 10000\n",
    "test_labels = np.genfromtxt('../data/test_labels.csv', delimiter=',', dtype=\"int8\")\n",
    "test_imgs = np.empty((n_test,32,32,3))\n",
    "for i in range(n_test):\n",
    "    img_fn = f'../data/test_images/test{i+1:05d}.png'\n",
    "    test_imgs[i,:,:,:]=cv2.cvtColor(cv2.imread(img_fn),cv2.COLOR_BGR2RGB)\n",
    "evaluation(baseline_model, test_labels, test_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banned-bobby",
   "metadata": {},
   "source": [
    "The overall accuracy is $0.24$, which is better than random guess (which should have a accuracy around $0.10$). For the project, you should try to improve the performance by the following strategies:\n",
    "\n",
    "- Consider a better choice of model architectures, hyperparameters, or training scheme for the predictive model;\n",
    "- Use both `clean_noisy_trainset` and `noisy_trainset` for model training via **weakly supervised learning** methods. One possible solution is to train a \"label-correction\" model using the former, correct the labels in the latter, and train the final predictive model using the corrected dataset.\n",
    "- Apply techniques such as $k$-fold cross validation to avoid overfitting;\n",
    "- Any other reasonable strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-spring",
   "metadata": {},
   "source": [
    "# 4. Save Label Predictions CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-pulse",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load trained Model 1/2 networks\n",
    "model1 = modelI() \n",
    "model1.load_weights(\"../output/modelI.h5\")\n",
    "\n",
    "model2 = modelI()\n",
    "model2.load_weights(\"../output/modelII.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-military",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "#Load test image data \n",
    "test_fp = \"../data/images/*\"\n",
    "test_img_files = glob.glob(test_fp)\n",
    "test_img_files = test_img_files[:10000]\n",
    "\n",
    "test_imgs = np.empty((10000,32,32,3))\n",
    "i=0\n",
    "for f in test_img_files: \n",
    "    test_imgs[i,:,:,:]=cv2.cvtColor(cv2.imread(f),cv2.COLOR_BGR2RGB)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-warrior",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline Predictions \n",
    "baseline_pred = []\n",
    "for im in test_imgs: \n",
    "    baseline_pred.append(baseline_model(im)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-sierra",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model I\n",
    "model1_pred = model1.predict(data_genI.flow(test_imgs))\n",
    "model1_pred = np.argmax(model1_pred, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-reference",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model II\n",
    "model2_pred = model2.predict(data_genII.flow(test_imgs))\n",
    "model2_pred = np.argmax(model2_pred, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-second",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save csv file \n",
    "import pandas as pd \n",
    "\n",
    "pred = read_csv(\"../output/label_prediction.csv\")\n",
    "\n",
    "pred['Baseline'] = baseline_pred\n",
    "pred['Model I'] = model1_pred\n",
    "pred[\"Model II\"] = model2_pred\n",
    "\n",
    "pred.to_csv(\"../output/label_prediction.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-pulse",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-toolbox",
   "metadata": {},
   "source": [
    "We ran our code on Google Colab and it works well with a RAM of 12.69 GB.\n",
    "\n",
    "Here are the basic structures of some models we tried. We modified layers and different values of the parameters to see which structure has a higher accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "national-petersburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artificial neural network (ANN) \n",
    "# model=Sequential()\n",
    "# model.add(Flatten(input_shape=(32,32,3)))\n",
    "# model.add(Dense(256,activation='relu'))\n",
    "# model.add(Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-cylinder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A multilayer perceptron (MLP)\n",
    "# model = Sequential()\n",
    "# model.add(Dense(256, activation='relu', input_dim=3072))\n",
    "# model.add(Dense(256, activation='relu'))\n",
    "# model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-pleasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutionary neural network(CNN)\n",
    "# model = Sequential()\n",
    "# model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(256, activation='relu'))\n",
    "# model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-diesel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer Learning: VGG 16, VGG 19, ResNet50, GoogLeNet and ArcFace\n",
    "# for example VGG19\n",
    "# base_model = VGG19(include_top=False,weights='imagenet',input_shape=(32,32,3),classes=y_train.shape[1])\n",
    "# base_model.trainable = False\n",
    "# img_input = Input(shape=(32, 32, 3))\n",
    "# model = base_model(img_input)\n",
    "# model = Flatten()(model)\n",
    "# model = Dense(512,activation=('relu'))(model)\n",
    "# out = Dense(10,activation=('softmax'))(model)\n",
    "# model = Model(img_input, out)\n",
    "\n",
    "# for example ResNet50\n",
    "# base_model = ResNet50(include_top=False,weights='imagenet',input_shape=(224,224,3),pooling='max')\n",
    "# base_model.trainable = False\n",
    "# img_input = Input(shape=(32,32,3))\n",
    "# model = UpSampling2D(size=(7,7))(img_input)\n",
    "# model = base_model_2(model)\n",
    "# model = Flatten()(model)\n",
    "# model = Dense(512, activation=\"relu\")(model)\n",
    "# out = Dense(10, activation=\"softmax\")(model)\n",
    "# model = Model(img_input, out)\n",
    "\n",
    "# for example ArcFace\n",
    "\n",
    "# base_model = ArcFaceModel(size=32, channels=3, num_classes=None, name='arcface_model',\n",
    "                 #margin=0.5, logist_scale=64, embd_shape=512,\n",
    "                 #head_type='ArcHead', backbone_type='ResNet50',\n",
    "                 #w_decay=5e-4, use_pretrain=True, training=False)\n",
    "# base_model.trainable = False\n",
    "# img_input = Input(shape=(32, 32, 3))\n",
    "# model = base_model(img_input)\n",
    "# model = Flatten()(model)\n",
    "# model = BatchNormalization()(model)\n",
    "# model = Dense(256, activation='relu')(model)\n",
    "# model = Dropout(0.3)(model)\n",
    "# model = BatchNormalization()(model)\n",
    "# model = Dense(128, activation='relu')(model)\n",
    "# model = Dropout(0.3)(model)\n",
    "# model = BatchNormalization()(model)\n",
    "# model = Dense(64, activation='relu')(model)\n",
    "# model = Dropout(0.3)(model)\n",
    "# out = Dense(10, activation='softmax')(model)\n",
    "# model = Model(img_input, out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automatic-president",
   "metadata": {},
   "source": [
    "To reduce overfitting, we tried some layers and methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confused-suicide",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for layers\n",
    "\n",
    "# Modify the parameter \"activity_regularizer\" of the dense layer\n",
    "# activity_regularizer = regularizers.l2(0.01)\n",
    "\n",
    "# Dropout layer randomly sets input units to 0 at the given rate for each step during training\n",
    "# Dropout(0.25)\n",
    "\n",
    "# Batch Normalization layer normalizes the inputs\n",
    "# BatchNormalization()\n",
    "\n",
    "# for methods\n",
    "# Early stopping stops training when a metric stops improving\n",
    "# early = EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "# when we are using transfer learning the overfitting is about 8%\n",
    "# we make some layers of the base model (such as the last fully connected layer)\n",
    "# trainable in order to reduce overfitting\n",
    "# base_model.get_layer('block5_conv4').trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modern-weather",
   "metadata": {},
   "source": [
    "We tried different optimizers and adjust the value of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-group",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent (with momentum) optimizer\n",
    "# sgd = SGD(learning_rate=0.001,momentum=.9,nesterov=False)\n",
    "\n",
    "# RMSprop\n",
    "# rms = RMSprop(learning_rate=0.001, rho=0.9, momentum=0.0, epsilon=1e-07, centered=False)\n",
    "\n",
    "# Adam\n",
    "# adam = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False)\n",
    "\n",
    "# Nadam\n",
    "# nadam = Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "\n",
    "# Adadelta\n",
    "# ada = Adadelta(learning_rate=0.1, rho=0.95, epsilon=1e-07)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-movement",
   "metadata": {},
   "source": [
    "For the learning rate of the optimizer, we tried learning rate schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funky-flash",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ExponentialDecay\n",
    "# an exponential decay schedule\n",
    "# lr_schedule = ExponentialDecay(initial_learning_rate=1e-2, decay_steps=10000, decay_rate=0.90)\n",
    "\n",
    "# ReduceLROnPlateau\n",
    "# this reduces learning rate when a metric stops improving\n",
    "# lrr= ReduceLROnPlateau( monitor='val_accuracy', factor=.01, patience=3, min_lr=1e-5)\n",
    "# callbacks = [lrr]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-floating",
   "metadata": {},
   "source": [
    "After adjusting model structure, tuning the values of hyperparameters and applying different methods, model I and model II are the best of all. The accuracy of ANN, MLP and CNN with other structures is about low 20s. The accuracy of transfer learning is about 35s but takes almost ten minutes per epoch."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
